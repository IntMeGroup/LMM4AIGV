<div align="center">

<h1>‚ù§Ô∏è LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation</h1>

</div>

<div align="center">

<h3>üí° How to evaluate Text-to-Video Generation Models properly?</h3>

<a href="https://huggingface.co/datasets/anonymousdb/AIGVE-60K">
   <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-green" alt="Hugging Face Dataset Badge"/>
</a>

</div>

<p align="center">
  <img width="1000" alt="LOVE" src="https://github.com/user-attachments/assets/20901a3b-68e8-4b65-89b1-acbe38ae7165" />
</p>

---


## ü§ó AIGVE-60K Download

[![ü§ó Hugging Face Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-green)](https://huggingface.co/datasets/anonymousdb/AIGVE-60K)

Download with CLI:

```bash
huggingface-cli download anonymousdb/AIGVE-60K --repo-type dataset --local-dir ./AIGVE-60K
```
</div>

<p align="center">
  <img width="1000" alt="LOVEe" src="https://github.com/user-attachments/assets/00a8e408-4480-4f93-9a24-d12db6b2d3da" />
</p>


|   Overall Rank | Model                                                                           |   Perception |   Perception Rank |   Correspondence |   Correspondence Rank |   TaskAccuracy |   TaskAccuracy Rank |
|---------------:|:--------------------------------------------------------------------------------|-------------:|------------------:|-----------------:|----------------------:|---------------:|--------------------:|
|              1 | ü•á [Pixverse](https://pixverse.ai/)                                             |        63.81 |                 2 |            59.97 |                     2 |          91.33 |                   1 |
|              2 | ü•à [Wanxiang](https://tongyi.aliyun.com/wanxiang/)                              |        60.54 |                 7 |            60.37 |                     1 |          90.33 |                   2 |
|              3 | ü•â [Hailuo](https://hailuoai.video/)                                            |        60.58 |                 5 |            59.74 |                     3 |          87.67 |                   3 |
|              4 | [Jimeng](https://jimeng.jianying.com/)                                          |        65.25 |                 1 |            57.86 |                     6 |          81.33 |                   6 |
|              5 | [Sora](https://openai.com/research/video-generation-models-as-world-simulators) |        62.09 |                 4 |            59.68 |                     4 |          85.67 |                   5 |
|              6 | [Hunyuan](https://aivideo.hunyuan.tencent.com/)                                 |        58.81 |                 9 |            57.25 |                     7 |          79.67 |                   7 |
|              7 | [Vidu1.5](https://www.vidu.studio/zh)                                           |        54.56 |                15 |            58.25 |                     5 |          87    |                   4 |
|              8 | [Gen3](https://runwayml.com/research/introducing-gen-3-alpha)                   |        59.22 |                 8 |            55.72 |                     8 |          75.33 |                   9 |
|              9 | [Kling](https://klingai.io/)                                                    |        60.56 |                 6 |            55.57 |                     9 |          73.67 |                  11 |
|             10 | [Genmo](https://www.genmo.ai)                                                   |        57.66 |                11 |            53.78 |                    11 |          75.67 |                   8 |
|             11 | [ChatGLM](https://chatglm.cn/video?lang=zh)                                     |        56.39 |                13 |            53.98 |                    10 |          74    |                  10 |
|             12 | [Xunfei](https://typemovie.art/)                                                |        58.6  |                10 |            53.46 |                    12 |          66.33 |                  12 |
|             13 | [Pyramid](https://github.com/jy0205/Pyramid-Flow)                               |        63.67 |                 3 |            50.17 |                    16 |          50.17 |                  22 |
|             14 | [Wan2.1](https://github.com/FoundationVision/LlamaGen)                          |        57.27 |                12 |            52.33 |                    13 |          62.67 |                  16 |
|             15 | [Allegro](https://github.com/rhymes-ai/Allegro)                                 |        56.08 |                14 |            50.7  |                    15 |          63    |                  15 |
|             16 | [VideoCrafter2](https://github.com/AILab-CVC/VideoCrafter)                      |        48.11 |                19 |            51.07 |                    14 |          65.67 |                  13 |
|             17 | [CogVideo X1.5](https://github.com/THUDM/CogVideo)                              |        50.59 |                16 |            49.73 |                    17 |          64.67 |                  14 |
|             18 | [Animate](https://github.com/aigc-apps/EasyAnimate)                             |        50.48 |                17 |            49.3  |                    18 |          60.67 |                  17 |
|             19 | [Lavie](https://github.com/Vchitect/LaVie)                                      |        49.3  |                18 |            48.22 |                    19 |          55    |                  20 |
|             20 | [Hotshot-XL](https://github.com/hotshotco/Hotshot-XL)                           |        42.66 |                22 |            47.75 |                    20 |          57.67 |                  18 |
|             21 | [Latte](https://github.com/Vchitect/Latte)                                      |        43.81 |                21 |            46.73 |                    22 |          54.33 |                  21 |
|             22 | [VideoCrafter1](https://github.com/AILab-CVC/VideoCrafter)                      |        44.12 |                20 |            44.67 |                    24 |          46    |                  25 |
|             23 | [Text2Video-Zero](https://github.com/Picsart-AI-Research/Text2Video-Zero)       |        40.53 |                24 |            44.89 |                    23 |          48.67 |                  23 |
|             24 | [NOVA](https://github.com/baaivision/NOVA)                                      |        41.18 |                23 |            47.18 |                    21 |          56    |                  19 |
|             25 | [ModelScope](https://github.com/modelscope/modelscope)                          |        38    |                26 |            43.73 |                    25 |          47.33 |                  24 |
|             26 | [Tune-A-Video](https://github.com/showlab/Tune-A-Video)                         |        35.41 |                27 |            42.69 |                    26 |          43    |                  26 |
|             27 | [LTX](https://github.com/Lightricks/LTX-Video)                                  |        40.11 |                25 |            41.28 |                    28 |          37    |                  28 |
|             28 | [LVDM](https://github.com/YingqingHe/LVDM)                                      |        33.84 |                28 |            42.2  |                    27 |          40.33 |                  27 |
|             29 | [ZeroScope](https://huggingface.co/cerspense/zeroscope_v2_XL)                   |        30.08 |                29 |            34.69 |                    29 |          22    |                  29 |
|             30 | [LWM](https://github.com/LargeWorldModel/LWM)                                   |        27.39 |                30 |            31.49 |                    30 |           9    |                  30 |
</div>

# ‚ù§Ô∏è LOVE Metric -- <u>L</u>MM f<u>o</u>r <u>V</u>ideo <u>E</u>valuation
<p align="center">
  <img width="1000" alt="model" src="https://github.com/user-attachments/assets/67b09453-2a50-4869-9843-4b1d089c8cf9" />
</p>


## ‚öôÔ∏è Installation

Clone the repository:

```bash
git clone https://github.com/IntMeGroup/LOVE.git
```

Create and activate a conda environment:

```bash
conda create -n LOVE python=3.9 -y
conda activate LOVE
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Install `flash-attn==2.3.6` (pre-built):

```bash
pip install flash-attn==2.3.6 --no-build-isolation
```

Or compile from source:

```bash
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
git checkout v2.3.6
python setup.py install
```

---

## üîß Preparation

### üìÅ Prepare dataset

```bash
huggingface-cli download anonymousdb/AIGVE-60K data.zip --repo-type dataset --local-dir ./
unzip data.zip -d ./data
```
### üì¶ Prepare model weights

```bash
huggingface-cli download OpenGVLab/InternVL3-9B --local_dir OpenGVLab/InternVL3-9B
huggingface-cli download anonymousdb/LOVE-pretrain temporal.pth ./
```

---



## üöÄ Training


### üìà Stage 1: Text-based quality training

```bash
sh shell/st1_train.sh
```

### üé® Stage 2: Fine-tune vision encoder and LLM with LoRA

```bash
sh shell/st2_train.sh
```

### ‚ùì Question-Answering (QA) Training

```bash
sh shell/train_qa.sh
```

---

## üöÄ Evaluation

### üì¶ Download pretrained weights

```bash
huggingface-cli download anonymousdb/LOVE-Perception --local-dir ./weights/stage2/stage2_mos1
huggingface-cli download anonymousdb/LOVE-Correspondence --local-dir ./weights/stage2/stage2_mos2
huggingface-cli download anonymousdb/LOVE-QA --local-dir ./weights/qa
```

### üìà Evaluate perception & correspondence scores

[![HF](https://img.shields.io/badge/%F0%9F%A4%97%20LOVE--Perception-orange)](https://huggingface.co/anonymousdb/LOVE-Perception)  
[![HF](https://img.shields.io/badge/%F0%9F%A4%97%20LOVE--Correspondence-green)](https://huggingface.co/anonymousdb/LOVE-Correspondence)

```bash
sh shell/eval_score.sh
```

### ‚ùì Evaluate question-answering

[![HF](https://img.shields.io/badge/%F0%9F%A4%97%20LOVE--QA-blue)](https://huggingface.co/anonymousdb/LOVE-QA)

```bash
sh shell/eval_qa.sh
```

---
# üé• Text-to-Video (T2V) Generation Models

This section lists 30 representative T2V generation models, including both **commercial close-source models** and **open-source lab models**, with links to their official or GitHub pages.

---

## ‚ô†Ô∏è Close-Source Commercial T2V Models

| Model | URL |
|-------|-----|
| **Pixverse** | [https://pixverse.ai/](https://pixverse.ai/) |
| **Wanxiang** | [https://tongyi.aliyun.com/wanxiang/](https://tongyi.aliyun.com/wanxiang/) |
| **Hailuo** | [https://hailuoai.video/](https://hailuoai.video/) |
| **Jimeng** | [https://jimeng.jianying.com/](https://jimeng.jianying.com/) |
| **Sora** | [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators) |
| **Hunyuan** | [https://aivideo.hunyuan.tencent.com/](https://aivideo.hunyuan.tencent.com/) |
| **Vidu1.5** | [https://www.vidu.studio/zh](https://www.vidu.studio/zh) |
| **Gen3** | [https://runwayml.com/research/introducing-gen-3-alpha](https://runwayml.com/research/introducing-gen-3-alpha) |
| **Kling** | [https://klingai.io/](https://klingai.io/) |
| **Genmo** | [https://www.genmo.ai](https://www.genmo.ai) |
| **ChatGLM** | [https://chatglm.cn/video?lang=zh](https://chatglm.cn/video?lang=zh) |
| **Xunfei** | [https://typemovie.art/](https://typemovie.art/) |

---

## ‚ù§Ô∏è Open-Source Lab T2V Models

| Model | URL |
|-------|-----|
| **Pyramid** | [https://github.com/jy0205/Pyramid-Flow](https://github.com/jy0205/Pyramid-Flow) |
| **Wan2.1** | [https://github.com/FoundationVision/LlamaGen](https://github.com/FoundationVision/LlamaGen) |
| **Allegro** | [https://github.com/rhymes-ai/Allegro](https://github.com/rhymes-ai/Allegro) |
| **VideoCrafter2** | [https://github.com/AILab-CVC/VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) |
| **CogVideo X1.5** | [https://github.com/THUDM/CogVideo](https://github.com/THUDM/CogVideo) |
| **Animate** | [https://github.com/aigc-apps/EasyAnimate](https://github.com/aigc-apps/EasyAnimate) |
| **Lavie** | [https://github.com/Vchitect/LaVie](https://github.com/Vchitect/LaVie) |
| **Hotshot-XL** | [https://github.com/hotshotco/Hotshot-XL](https://github.com/hotshotco/Hotshot-XL) |
| **Latte** | [https://github.com/Vchitect/Latte](https://github.com/Vchitect/Latte) |
| **VideoCrafter1** | [https://github.com/AILab-CVC/VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) |
| **Text2Video-Zero** | [https://github.com/Picsart-AI-Research/Text2Video-Zero](https://github.com/Picsart-AI-Research/Text2Video-Zero) |
| **NOVA** | [https://github.com/baaivision/NOVA](https://github.com/baaivision/NOVA) |
| **ModelScope** | [https://github.com/modelscope/modelscope](https://github.com/modelscope/modelscope) |
| **Tune-A-Video** | [https://github.com/showlab/Tune-A-Video](https://github.com/showlab/Tune-A-Video) |
| **LTX** | [https://github.com/Lightricks/LTX-Video](https://github.com/Lightricks/LTX-Video) |
| **LVDM** | [https://github.com/YingqingHe/LVDM](https://github.com/YingqingHe/LVDM) |
| **ZeroScope** | [https://huggingface.co/cerspense/zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) |
| **LWM** | [https://github.com/LargeWorldModel/LWM](https://github.com/LargeWorldModel/LWM) |

---





# üìä V2T Interpretation Model Collection

This repository provides a comprehensive list of ** Vision-to-Text (V2T) interpretation models**, covering conventional video quality assessment models, learning-based image-text alignment models, large multimodal models (LMMs), and proprietary foundation models. Each method is annotated with its category and accompanied by a corresponding GitHub or official URL.

---

## üß≠ Categories
- <img src="https://upload.wikimedia.org/wikipedia/commons/2/21/Matlab_Logo.png" alt="Hugging Face" width="22"/> Conventional VQA Metrics
- ‚ô£Ô∏è Classical VQA Models  
- ‚ù§Ô∏è Learning-based Scoring Models  
- ‚≠ê Large Multimodal Models (LMMs)  
- üî∫ Proprietary Foundation Models (Closed-source)

---
## <img src="https://upload.wikimedia.org/wikipedia/commons/2/21/Matlab_Logo.png" alt="Hugging Face" width="27"/> Conventional VQA Metrics
For
**BMPRI, BPRI, BRISQUE, HOSA, NIQE, QAC**
run:
```
videobench.m
```

## üìö Model List and URLs

| Category | Method | URL |
|----------|--------|-----|
| ‚ô£Ô∏è | **VSFA** | [GitHub](https://github.com/lidq92/VSFA) |
| ‚ô£Ô∏è | **BVQA** | [GitHub](https://github.com/vztu/BVQA_Benchmark) |
| ‚ô£Ô∏è | **SimpleVQA** | [GitHub](https://github.com/Raykshj/SimpleVQA) |
| ‚ô£Ô∏è | **FAST-VQA** | [GitHub](https://github.com/VQAssessment/FAST-VQA-and-FasterVQA) |
| ‚ô£Ô∏è | **DOVER** | [GitHub](https://github.com/VQAssessment/DOVER) |
| ‚ù§Ô∏è | **CLIPScore** | [GitHub](https://github.com/jmhessel/clipscore) |
| ‚ù§Ô∏è | **BLIPScore** | [GitHub](https://github.com/salesforce/BLIP) |
| ‚ù§Ô∏è | **AestheticScore** | [GitHub](https://github.com/sorekdj60/AestheticScore) |
| ‚ù§Ô∏è | **ImageReward** | [GitHub](https://github.com/THUDM/ImageReward) |
| ‚ù§Ô∏è | **PickScore** | [GitHub](https://github.com/yuvalkirstain/PickScore) |
| ‚ù§Ô∏è | **HPSv2** | [GitHub](https://github.com/tgxs002/HPSv2) |
| ‚ù§Ô∏è | **VQAScore** | [GitHub](https://github.com/linzhiqiu/t2v_metrics) |
| ‚ù§Ô∏è | **FGA-BLIP2** | [GitHub](https://github.com/DYEvaLab/EvalMuse) |
| ‚≠ê | **DeepSeek-VL2** | [GitHub](https://github.com/deepseek-ai/DeepSeek-V2) |
| ‚≠ê | **Video-LLaVA** | [GitHub](https://github.com/PKU-YuanGroup/Video-LLaVA) |
| ‚≠ê | **VideoLLaMA3** | [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA3) |
| ‚≠ê | **mPLUG-OWL3** | [GitHub](https://github.com/X-PLUG/mPLUG-Owl) |
| ‚≠ê | **Qwen2.5-VL** | [GitHub](https://github.com/QwenLM/Qwen2.5-VL) |
| ‚≠ê | **LLaMA-3.2-Vision** | [HuggingFace](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision) |
| ‚≠ê | **CogAgent** | [GitHub](https://github.com/THUDM/CogAgent) |
| ‚≠ê | **LLaVA-NeXT** | [GitHub](https://github.com/LLaVA-VL/LLaVA-NeXT) |
| ‚≠ê | **InternVideo2.5** | [GitHub](https://github.com/OpenGVLab/InternVideo) |
| ‚≠ê | **InternVL** | [GitHub](https://github.com/OpenGVLab/InternVL) |
| üî∫ | **Gemini 1.5 Pro** | [Official](https://gemini.google.com) |
| üî∫ | **Claude 3.5** | [Official](https://claude.ai) |
| üî∫ | **Grok2 Vision** | [Official](https://grok.com) |
| üî∫ | **ChatGPT-4o** | [Official](https://chatgpt.com) |

---

| üèÜ Rank | Model | Perception SRCC | üß† Rank | Correspondence SRCC | üîó Rank | QA Accuracy (%) | ‚úÖ Rank |
|--------|--------|------------------|--------|----------------------|--------|------------------|--------|
| ü•á 1 | **LOVE** | 0.7932 | 1 | 0.7466 | 1 | 78.69 | 1 |
| ü•à 2 | **InternVL2.5 (38B)** | 0.6227 | 6 | 0.647 | 4 | 75.81 | 3 |
| ü•â 3 | **Grok2 Vision** | 0.5628 | 11 | 0.6659 | 2 | 76.51 | 2 |
| 4 | InternVL3 (72B) | 0.5441 | 13 | 0.6314 | 5 | 74.59 | 5 |
| 5 | InternVL2.5 (72B) | 0.5383 | 16 | 0.6612 | 3 | 75.18 | 4 |
| 6 | Gemini1.5-pro | 0.4972 | 20 | 0.6095 | 7 | 73.38 | 8 |
| 7 | InternVL3 (38B) | 0.495 | 21 | 0.5996 | 8 | 73.89 | 6 |
| 8 | Llava-one-vision (72B) | 0.5291 | 17 | 0.5702 | 11 | 73.31 | 9 |
| 9 | Qwen2.5-VL (72B) | 0.4245 | 25 | 0.6272 | 6 | 73.83 | 7 |
| 10 | Claude3.5 | 0.4267 | 24 | 0.5827 | 10 | 73.2 | 10 |
| 11 | FGA-BLIP2 | 0.5181 | 18 | 0.5962 | 9 | 67.06 | 19 |
| 12 | Qwen2-VL (72B) | 0.4628 | 23 | 0.5598 | 12 | 73.12 | 11 |
| 13 | HPSv2 | 0.5415 | 14 | 0.4989 | 16 | 67.68 | 18 |
| 14 | FAST-VQA | 0.6391 | 5 | 0.3919 | 28 | 66.27 | 21 |
| 15 | HOSA | 0.6474 | 3 | 0.4153 | 23 | 64.34 | 28 |
| 16 | ImageReward | 0.418 | 26 | 0.5076 | 15 | 68.33 | 17 |
| 17 | QAC | 0.5958 | 7 | 0.3948 | 26 | 64.4 | 27 |
| 18 | NIQE | 0.6536 | 2 | 0.4345 | 21 | 62.21 | 38 |
| 19 | BRISQUE | 0.5843 | 8 | 0.3806 | 29 | 64.67 | 25 |
| 20 | AestheticScore | 0.5524 | 12 | 0.3931 | 27 | 64.87 | 24 |
| 21 | Qwen2-VL (7B) | 0.3568 | 31 | 0.4498 | 20 | 71.56 | 12 |
| 22 | Qwen2.5-VL (7B) | 0.541 | 15 | 0.511 | 14 | 62.34 | 36 |
| 23 | VideoLlama3 (8B) | 0.3922 | 29 | 0.4228 | 22 | 70.16 | 15 |
| 24 | DOVER | 0.6414 | 4 | 0.3759 | 30 | 62.61 | 34 |
| 25 | BMPRI | 0.5741 | 9 | 0.3618 | 31 | 64 | 29 |
| 26 | V-Aesthetic Quality | 0.5031 | 19 | 0.4033 | 25 | 64.54 | 26 |
| 27 | LLaVA-NeXT (8B) | 0.4888 | 22 | 0.2847 | 35 | 70.21 | 14 |
| 28 | InternVideo2.5 (8B) | 0.1563 | 42 | 0.4978 | 17 | 70.64 | 13 |
| 29 | InternVL2.5 (8B) | 0.2799 | 37 | 0.4856 | 18 | 66.3 | 20 |
| 30 | mPLUG-Owl3 (7B) | 0.3532 | 33 | 0.5478 | 13 | 63.02 | 33 |
| 31 | InternVL3 (9B) | 0.2731 | 38 | 0.4768 | 19 | 65.82 | 22 |
| 32 | SimpleVQA | 0.5631 | 10 | 0.3474 | 32 | 60.78 | 41 |
| 33 | PickScore | 0.4026 | 28 | 0.4135 | 24 | 62.29 | 37 |
| 34 | VideoLlava (7B) | 0.1809 | 40 | 0.2005 | 40 | 68.46 | 16 |
| 35 | V-Temporal Flickering | 0.4076 | 27 | 0.1958 | 41 | 63.69 | 31 |
| 36 | BLIPScore | 0.1884 | 39 | 0.3163 | 33 | 63.93 | 30 |
| 37 | BPRI | 0.3558 | 32 | 0.2018 | 39 | 63.56 | 32 |
| 38 | VSFA | 0.375 | 30 | 0.2438 | 36 | 57.09 | 45 |
| 39 | CogAgent (18B) | 0.1244 | 44 | 0.119 | 45 | 65.32 | 23 |
| 40 | V-Subject Consistency | 0.3443 | 34 | 0.1647 | 44 | 62.52 | 35 |
| 41 | BVQA | 0.3089 | 35 | 0.2379 | 37 | 58.47 | 43 |
| 42 | V-Overall Consistency | 0.1559 | 43 | 0.3076 | 34 | 61.96 | 40 |
| 43 | V-Imaging Quality | 0.281 | 36 | 0.1952 | 42 | 60.6 | 42 |
| 44 | CLIPScore | 0.0947 | 45 | 0.229 | 38 | 58.27 | 44 |
| 45 | VQAScore | 0.1677 | 41 | 0.1763 | 43 | 52.97 | 46 |
| 46 | Llama3.2-Vision (11B) | 0.094 | 46 | 0.0804 | 46 | 62.19 | 39 |
| 47 | DeepseekVL2 (1B) | 0.0121 | 47 | 0.0173 | 47 | 39.29 | 47 |


![image](https://github.com/user-attachments/assets/23462263-d4dd-406c-9366-d9572670785d)

## ‚≠ê Acknowledgements

Thanks to the original authors of all the models listed here. This is a curated list intended to help researchers and developers in the **T2V generation**,**V2T interpretation** and **multimodal quality assessment** communities.



## üìå TODO
- ‚úÖ Release the training code 
- ‚úÖ Release the evaluation code 
- ‚úÖ Release the AIGVE-60K Database

