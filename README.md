# 

<div align="center">
   <h1>‚ù§Ô∏èLOVEÔºöBenchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation</h1>
   <i>How to evaluate Text to Video Generation Model properly?</i>
   <div>
      <a href="https://huggingface.co/datasets/anonymousdb/AIGVE-60K"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-green"></a>
   </div>
</div>
<img width="1000" alt="LOVE" src="https://github.com/user-attachments/assets/20901a3b-68e8-4b65-89b1-acbe38ae7165" />


## <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" width="27"/> AIGVE-60K Download
 <a href="https://huggingface.co/datasets/anonymousdb/AIGVE-60K"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-green"></a>
```
huggingface-cli download anonymousdb/AIGVE-60K --repo-type dataset --local-dir ./AIGVE-60K
```
## üõ†Ô∏è Installation

Clone this repository:
```
git clone https://github.com/IntMeGroup/LOVE.git
```
Create a conda virtual environment and activate it:
```
conda create -n LOVE python=3.9 -y
conda activate LOVE
```
Install dependencies using requirements.txt:
```
pip install -r requirements.txt
```
Install flash-attn==2.3.6
```
pip install flash-attn==2.3.6 --no-build-isolation
```
Alternatively you can compile from source:
```
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
git checkout v2.3.6
python setup.py install
```

## üåà Training
Preparation
```
huggingface-cli download anonymousdb/AIGVE-60K/data --repo-type dataset --local-dir ./data
```

for stage1 training (Text-based quality levels)

```
sh shell/st1_train.sh
```
for stage2 training (Fine-tuning the vision encoder and LLM with LoRA)

```
sh shell/st2_train.sh
```

for quastion-answering training (QA)
```
sh shell/train_qa.sh
```

## üåà Evaluation
Download the pretrained weights
```
huggingface-cli download anonymousdb/LOVE-Perception --local-dir ./weights/stage2/stage2_mos1
huggingface-cli download anonymousdb/LOVE-Correspondence --local-dir ./weights/stage2/stage2_mos2
huggingface-cli download anonymousdb/LOVE-QA --local-dir ./weights/qa
```

for [perception](https://huggingface.co/anonymousdb/LOVE-Perception) and [correspondence](https://huggingface.co/anonymousdb/LOVE-Correspondence) score evaluation (Scores)

```
sh shell/eval_score.sh
```

for [quastion-answering](https://huggingface.co/anonymousdb/LOVE-QA) evaluation (QA)
```
sh shell/eval_qa.sh
```
## <img src="https://upload.wikimedia.org/wikipedia/commons/2/21/Matlab_Logo.png" alt="Hugging Face" width="27"/> Conventional VQA Metrics
For
**BMPRI, BPRI, BRISQUE, HOSA, NIQE, QAC**
run:
```
videobench.m
```
# üìä V2T Interpretation Model Collection

This repository provides a comprehensive list of **30 Vision-to-Text (V2T) interpretation models**, covering traditional video quality assessment models, learning-based image-text alignment models, large multimodal models (LMMs), and proprietary foundation models. Each method is annotated with its category and accompanied by a corresponding GitHub or official URL.

---

## üß≠ Categories

- ‚ô£Ô∏è Classical VQA Models  
- ‚ù§Ô∏è Learning-based Scoring Models  
- ‚≠ê Large Multimodal Models (LMMs)  
- üî∫ Proprietary Foundation Models (Closed-source)

---

## üìö Model List and URLs

| Category | Method | URL |
|----------|--------|-----|
| ‚ô£Ô∏è | **VSFA** [[paper]](https://arxiv.org/abs/1908.02463) | [GitHub](https://github.com/lidq92/VSFA) |
| ‚ô£Ô∏è | **BVQA** | [GitHub](https://github.com/vztu/BVQA_Benchmark) |
| ‚ô£Ô∏è | **SimpleVQA** | [GitHub](https://github.com/Raykshj/SimpleVQA) |
| ‚ô£Ô∏è | **FAST-VQA** | [GitHub](https://github.com/VQAssessment/FAST-VQA-and-FasterVQA) |
| ‚ô£Ô∏è | **DOVER** | [GitHub](https://github.com/VQAssessment/DOVER) |
| ‚ù§Ô∏è | **CLIPScore** | [GitHub](https://github.com/jmhessel/clipscore) |
| ‚ù§Ô∏è | **BLIPScore** | [GitHub](https://github.com/salesforce/BLIP) |
| ‚ù§Ô∏è | **AestheticScore** | [GitHub](https://github.com/sorekdj60/AestheticScore) |
| ‚ù§Ô∏è | **ImageReward** | [GitHub](https://github.com/THUDM/ImageReward) |
| ‚ù§Ô∏è | **PickScore** | [GitHub](https://github.com/yuvalkirstain/PickScore) |
| ‚ù§Ô∏è | **HPSv2** | [GitHub](https://github.com/tgxs002/HPSv2) |
| ‚ù§Ô∏è | **VQAScore** | [GitHub](https://github.com/linzhiqiu/t2v_metrics) |
| ‚ù§Ô∏è | **FGA-BLIP2** | [GitHub](https://github.com/DYEvaLab/EvalMuse) |
| ‚≠ê | **DeepSeek-VL2** | [GitHub](https://github.com/deepseek-ai/DeepSeek-V2) |
| ‚≠ê | **Video-LLaVA** | [GitHub](https://github.com/PKU-YuanGroup/Video-LLaVA) |
| ‚≠ê | **VideoLLaMA3** | [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA3) |
| ‚≠ê | **mPLUG-OWL3** | [GitHub](https://github.com/X-PLUG/mPLUG-Owl) |
| ‚≠ê | **Qwen2.5-VL** | [GitHub](https://github.com/QwenLM/Qwen2.5-VL) |
| ‚≠ê | **LLaMA-3.2-Vision** | [HuggingFace](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision) |
| ‚≠ê | **CogAgent** | [GitHub](https://github.com/THUDM/CogAgent) |
| ‚≠ê | **LLaVA-NeXT** | [GitHub](https://github.com/LLaVA-VL/LLaVA-NeXT) |
| ‚≠ê | **InternVideo2.5** | [GitHub](https://github.com/OpenGVLab/InternVideo) |
| ‚≠ê | **InternVL** | [GitHub](https://github.com/OpenGVLab/InternVL) |
| üî∫ | **Gemini 1.5 Pro** | [Official](https://gemini.google.com) |
| üî∫ | **Claude 3.5** | [Official](https://claude.ai) |
| üî∫ | **Grok2 Vision** | [Official](https://grok.com) |
| üî∫ | **ChatGPT-4o** | [Official](https://chatgpt.com) |

---

## üìå Citation Notice

If you use this list or part of the benchmark in your research, please consider citing the original papers listed above.

---

## üîß Contributions

If you'd like to contribute a new model, open an issue or submit a pull request with:

- Model name
- URL (GitHub or official site)
- Paper reference (if available)
- Category suggestion

---

## ‚≠ê Acknowledgements

Thanks to the original authors of all the models listed here. This is a curated list intended to help researchers and developers in the **V2T interpretation** and **multimodal quality assessment** communities.



## üìå TODO
- ‚úÖ Release the training code 
- ‚úÖ Release the evaluation code 
- ‚úÖ Release the AIGVE-60K Database

